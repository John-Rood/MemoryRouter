# MemoryRouter: Durable Objects Architecture

**The edge-native AI memory platform. No Python. No Cloud Run. No external databases. Just Workers + Durable Objects + R2.**

---

## The Core Insight

VectorVault TypeScript uses native Float32Array vectors with brute-force kNN search. No FAISS. No Annoy. No native bindings. No index rebuild step.

**You add a vector â†’ it's immediately searchable.** That's the edge advantage.

Durable Objects let us keep those vectors **alive in memory between requests**. First request loads them. Every request after that? Sub-millisecond search. No reload. No cold start.

---

## Architecture

```
Customer Request
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Cloudflare Worker (Edge)            â”‚
â”‚                                             â”‚
â”‚  1. Authenticate (mk_xxx memory key)        â”‚
â”‚  2. Parse model + provider                  â”‚
â”‚  3. KRONOS decides which vaults to query    â”‚
â”‚  4. Fan out to Durable Objects (parallel)   â”‚
â”‚  5. Merge results                           â”‚
â”‚  6. Format context for target model         â”‚
â”‚  7. Forward to AI provider (user's API key) â”‚
â”‚  8. Stream response back                    â”‚
â”‚  9. Store new memories in vault DO          â”‚
â”‚  10. Meter usage (memory tokens)            â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ parallel queries
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Durable Objects (Per Vault)           â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  core-{memory_key}                     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Float32Arrays in memory (HOT)     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ SQLite persistence (WARM)         â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Base knowledge, always loaded     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  session-{memory_key}-{session_id}     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Float32Arrays in memory (HOT)     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ SQLite persistence (WARM)         â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Per-user/conversation context     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ephemeral-{memory_key}-{conv_id}      â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Float32Arrays in memory (HOT)     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Short-lived, auto-expires         â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Single conversation context       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ cold backup (async)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              R2 (Cold Storage)                â”‚
â”‚  â””â”€â”€ Vault snapshots as JSONL (backup/export)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Storage Tiers

| Tier | Where | Latency | When |
|------|-------|---------|------|
| **HOT** | DO memory â€” Float32Arrays loaded | **< 1ms** | Vault is active (DO alive) |
| **WARM** | DO SQLite â€” built-in persistent storage | **10-50ms** | First request wakes DO, loads vectors |
| **COLD** | R2 â€” JSONL vault snapshots | **50-200ms** | Backup, export, disaster recovery |

The magic: **Durable Objects stay alive between requests.** Once a vault's vectors are loaded into memory, they stay there. Cloudflare manages hibernation automatically â€” when a DO goes idle, it hibernates. When the next request comes, it wakes up and reloads from SQLite.

---

## Why Durable Objects (Not KV + R2)

| | KV + R2 (current) | Durable Objects |
|---|---|---|
| **Vector search** | Load from KV every request | Already in memory |
| **First request** | ~60ms (KV fetch) | ~10-50ms (SQLite load) |
| **Repeat requests** | ~60ms (KV fetch again) | **< 1ms** (already loaded) |
| **Write** | Serialize â†’ KV put (~60ms) | In-memory + SQLite write |
| **Consistency** | Eventually consistent | **Strongly consistent** |
| **Isolation** | Shared KV namespace | Per-vault instance |
| **Coordination** | None (race conditions) | Single-threaded per DO |

The killer advantage: **KV re-fetches every request. DOs stay hot.** For a product where users send multiple requests per session, the DO stays alive and every search is sub-millisecond.

---

## KRONOS on Durable Objects

KRONOS retrieves memory across temporal windows:

| Window | Duration | Purpose |
|--------|----------|---------|
| **HOT** | 4 hours | What we just talked about |
| **WORKING** | 3 days | Recent conversation history |
| **LONG-TERM** | 90 days | Full knowledge base |

Each vault DO stores timestamps with every vector. KRONOS filtering happens **inside the DO** â€” the Worker tells the DO "give me top-4 results from the last 4 hours" and the DO's `searchFast()` method handles timestamp filtering on the already-loaded Float32Arrays.

```
Worker: "Search core vault, HOT window (last 4hrs), top 4"
   â†’ core DO: searches in-memory vectors with timestamp filter â†’ returns 4 results

Worker: "Search core vault, WORKING window (last 3 days), top 4"
   â†’ core DO: searches with different timestamp filter â†’ returns 4 results

Worker: "Search session vault, all windows, top 4"
   â†’ session DO: searches in-memory vectors â†’ returns 4 results

Worker: merges all results â†’ formats for model â†’ forwards to provider
```

---

## What We Already Have

| Component | Status | Notes |
|-----------|--------|-------|
| `WorkersVectorIndex` | âœ… Done | Pure TS, Float32Array, serialize/deserialize, searchFast, KRONOS time filtering |
| `StorageManager` | ğŸ”„ Refactor | Currently KV+R2, needs to become DO-aware |
| Worker routes | âœ… Done | Hono app, /v1/chat/completions, /v1/messages |
| Auth middleware | âœ… Done | mk_xxx validation |
| Memory middleware | âœ… Done | Memory injection/storage |
| Provider routing | âœ… Done | OpenAI, Anthropic forwarding |
| Model formatters | âœ… Done | Per-model context formatting |
| KRONOS | âœ… Done | 4-window temporal retrieval |
| Billing | âœ… Done | Stripe integration, memory token metering |

**What needs to be built:**
- `VaultDurableObject` class (wraps WorkersVectorIndex with SQLite persistence)
- DO routing logic (Worker â†’ correct DO based on memory key + vault type)
- DO â†” Worker communication protocol
- Migration from KV+R2 to DO storage
- Wrangler config updates for DO bindings

---

## The Stack

```
Language:       TypeScript (100%)
Runtime:        Cloudflare Workers
Persistence:    Durable Objects (SQLite)
Cold Storage:   R2 (JSONL snapshots)
Vectors:        VectorVault TS (Float32Array, brute-force kNN)
Auth:           JWT + memory keys (mk_xxx)
Billing:        Stripe (memory token metering)
AI Providers:   OpenAI, Anthropic, Google, OpenRouter (BYOK)
Framework:      Hono
```

No Python. No Redis. No GCS. No Cloud Run. No native bindings. No external vector database.

**One codebase. One language. Edge-native. Sub-millisecond memory.**

---

## The Business Model

MemoryRouter charges for **memory tokens** â€” not inference.

Users bring their own API keys (BYOK). They pay OpenAI/Anthropic directly for inference. They pay us for memory: storing, retrieving, and managing context across models.

**$1.00 per 1M memory tokens.** That's it.

The same memory works with **any model**. Start with Claude, switch to GPT, iterate with Haiku â€” all share the same context. Nobody else does this.

---

*MemoryRouter: Same memory, any model. Built at the edge. âš¡ğŸ§ *

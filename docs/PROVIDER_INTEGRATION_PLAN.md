# Provider Integration Plan: Preserving the Core

## ğŸ›¡ï¸ THE CORE (Non-Negotiable)

What we have NOW that must be preserved:

```
âœ… 55ms overhead for full memory processing
âœ… Auth validation
âœ… Vector search (DO + D1 fallback)
âœ… Buffer fetch (bundled with search)
âœ… Context injection
âœ… Context-window-aware truncation
âœ… 100% success rate on memory operations
```

**Rule #1: CORE COMES FIRST. Any changes must preserve these numbers.**

---

## ğŸ“Š Research: Build vs. Model from Library

### Option A: Use a Library (unified-llm, multi-llm-ts, abso-ai, etc.)

**Pros:**
- Less code to write
- Community maintained
- Handles edge cases we haven't hit yet

**Cons:**
- âš ï¸ **Dependency risk** â€” library updates could break our core
- âš ï¸ **Black box** â€” hard to debug when things break
- âš ï¸ **Feature mismatch** â€” libraries optimize for different use cases (chat, not memory)
- âš ï¸ **Streaming differences** â€” each library handles streaming differently
- âš ï¸ **Bundle size** â€” adds weight to our edge worker

**Verdict:** TOO RISKY for our core. These libraries are designed for general chat, not memory injection.

---

### Option B: Build Our Own (Provider Packages Directly)

**Pros:**
- âœ… Full control
- âœ… No dependency surprises
- âœ… Optimized for OUR use case (memory injection)
- âœ… Can preserve exact request/response format
- âœ… Easier to debug

**Cons:**
- More initial work
- Need to handle provider-specific quirks
- Maintenance burden for new models

**Verdict:** SAFER. We control the code, we control the risk.

---

## ğŸ” Current State Analysis

We ALREADY have formatters for:

| Provider | Request Transform | Response Transform | Streaming | Status |
|----------|-------------------|-------------------|-----------|--------|
| OpenAI | âœ… Native | âœ… Native | âœ… Native | **WORKING** |
| Anthropic | âœ… Custom | âœ… Custom | âœ… Custom | **WORKING** |
| Google/Gemini | âœ… Custom | âœ… Custom | âœ… Custom | **WORKING** |
| OpenRouter | âœ… OpenAI-compat | âœ… OpenAI-compat | âœ… OpenAI-compat | **WORKING** |
| xAI/Grok | âœ… OpenAI-compat | âœ… OpenAI-compat | âœ… OpenAI-compat | **WORKING** |
| Cerebras | âœ… OpenAI-compat | âœ… OpenAI-compat | âœ… OpenAI-compat | **WORKING** |

**The infrastructure already exists.** The issue is it's scattered and inconsistent.

---

## ğŸ¯ Target Providers

| Provider | API Format | Streaming Format | Complexity |
|----------|------------|------------------|------------|
| 1. OpenAI | Native | SSE (OpenAI) | Low |
| 2. Anthropic | Custom | SSE (Custom) | Medium |
| 3. Google/Gemini | Custom | SSE (Custom) | Medium |
| 4. xAI/Grok | OpenAI-compat | SSE (OpenAI) | Low |
| 5. Cerebras | OpenAI-compat | SSE (OpenAI) | Low |
| 6. OpenRouter | OpenAI-compat | SSE (OpenAI) | Low |

**Key insight:** 4 of 6 providers are OpenAI-compatible. Only Anthropic and Gemini need custom formatters.

---

## ğŸ—ï¸ Proposed Architecture

### Layer 1: Core Memory Engine (UNTOUCHED)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CORE MEMORY ENGINE                 â”‚
â”‚  - DO/D1 vector search                          â”‚
â”‚  - Buffer sync                                  â”‚
â”‚  - Truncation                                   â”‚
â”‚  - Context formatting                           â”‚
â”‚                                                 â”‚
â”‚  OUTPUT: augmentedMessages[], memoryMetadata    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
```

### Layer 2: Provider Adapter (NEW - Clean Interface)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PROVIDER ADAPTER LAYER               â”‚
â”‚                                                 â”‚
â”‚  INPUT: augmentedMessages[], model, options     â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ OpenAI   â”‚ â”‚Anthropic â”‚ â”‚ Google   â”‚        â”‚
â”‚  â”‚ Adapter  â”‚ â”‚ Adapter  â”‚ â”‚ Adapter  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   xAI    â”‚ â”‚ Cerebras â”‚ â”‚OpenRouterâ”‚        â”‚
â”‚  â”‚ Adapter  â”‚ â”‚ Adapter  â”‚ â”‚ Adapter  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                 â”‚
â”‚  OUTPUT: ProviderResponse (OpenAI format)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Each Adapter Implements:
```typescript
interface ProviderAdapter {
  name: string;
  
  // Transform OpenAI-format request â†’ provider format
  transformRequest(req: OpenAIRequest): ProviderRequest;
  
  // Transform provider response â†’ OpenAI format
  transformResponse(res: ProviderResponse): OpenAIResponse;
  
  // Handle streaming (return OpenAI-format SSE)
  transformStream(stream: ReadableStream): ReadableStream;
  
  // Build the fetch request
  buildRequest(transformed: ProviderRequest, apiKey: string): Request;
}
```

---

## ğŸ“‹ Implementation Plan

### Phase 1: Refactor Without Changing Behavior (Day 1)

**Goal:** Extract current provider logic into clean adapter files WITHOUT changing functionality.

1. Create `src/adapters/` directory
2. Create base `ProviderAdapter` interface
3. Extract OpenAI logic â†’ `adapters/openai.ts`
4. Extract Anthropic logic â†’ `adapters/anthropic.ts`
5. Extract Google logic â†’ `adapters/google.ts`
6. Create OpenAI-compat base â†’ `adapters/openai-compat.ts`
7. xAI, Cerebras, OpenRouter extend OpenAI-compat

**Test:** All existing tests pass. Latency unchanged.

### Phase 2: Standardize Response Format (Day 2)

**Goal:** All providers return OpenAI-format responses.

1. Ensure Anthropic adapter returns OpenAI format
2. Ensure Google adapter returns OpenAI format
3. Standardize streaming SSE format
4. Add comprehensive tests for each provider

**Test:** Same response structure regardless of provider.

### Phase 3: Add Missing Providers (Day 3)

**Goal:** Fill any gaps in the 6 target providers.

1. Verify xAI/Grok works correctly
2. Verify Cerebras works correctly  
3. Add any missing model detection
4. Test all providers with real API calls

**Test:** All 6 providers working, same latency.

### Phase 4: Error Handling & Fallbacks (Day 4)

**Goal:** Graceful degradation.

1. Provider-specific error parsing
2. Rate limit handling
3. Timeout handling
4. Optional fallback to different provider

**Test:** Errors return meaningful messages, don't crash core.

---

## âš ï¸ Risk Mitigation

### What Could Break the Core:

| Risk | Mitigation |
|------|------------|
| New code adds latency | Benchmark before/after each change |
| Streaming breaks | Keep streaming path separate, test thoroughly |
| Memory injection skipped | Add assertion that memory was injected |
| Provider error crashes worker | Wrap all provider calls in try/catch |

### Safety Checks:

```typescript
// Add to every request
if (!augmentedMessages.some(m => m.role === 'system' && m.content.includes('Memory'))) {
  console.error('[SAFETY] Memory injection may have failed!');
}
```

---

## ğŸ“ Proposed File Structure

```
src/
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ index.ts           # Export all adapters, getAdapter()
â”‚   â”œâ”€â”€ base.ts            # ProviderAdapter interface
â”‚   â”œâ”€â”€ openai.ts          # OpenAI native
â”‚   â”œâ”€â”€ openai-compat.ts   # Base for OpenAI-compatible providers
â”‚   â”œâ”€â”€ anthropic.ts       # Anthropic Claude
â”‚   â”œâ”€â”€ google.ts          # Google Gemini
â”‚   â”œâ”€â”€ xai.ts             # xAI Grok (extends openai-compat)
â”‚   â”œâ”€â”€ cerebras.ts        # Cerebras (extends openai-compat)
â”‚   â””â”€â”€ openrouter.ts      # OpenRouter (extends openai-compat)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ providers.ts       # Provider detection (keep)
â”‚   â””â”€â”€ ...
â””â”€â”€ routes/
    â””â”€â”€ chat.ts            # Use adapters instead of inline transforms
```

---

## âœ… Success Criteria

1. **Latency:** mr_overhead stays â‰¤20ms on warm DO
2. **Reliability:** 100% success rate maintained
3. **Compatibility:** All 6 providers work with memory injection
4. **Response format:** All providers return OpenAI-format responses
5. **Streaming:** All providers stream correctly
6. **No regressions:** Existing tests still pass

---

## ğŸš€ Recommendation

**Build our own adapters using provider SDKs/APIs directly.**

Reasons:
1. We already have 80% of the code written
2. Libraries add unnecessary complexity and risk
3. Our use case (memory injection) is unique
4. Full control = full confidence

**Timeline:** 4 days to clean implementation, fully tested.

---

*Written: 2026-02-02 | Core comes first. Always.*
